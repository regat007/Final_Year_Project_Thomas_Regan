# Final_Year_Project_Thomas_Regan
This repository contains the code developed as part of my final year project. There are three models: lexicon-based, Support vector machine, and a BERT-based model for subjectivity detection. There is also the supplied lexicon and a data file (Note: The 'training_data.csv' is split and used for testing and training within the models). 

Instructions for running the models:

All models require Python 3.6+ installed and the following dependencies...
Core Python libraries: String, CSV, random
Machine Learning/Deep learning libraries: sklearn, nltk, transformers, pyTorch
Data processing libraries: NumPy, Pandas
Utility libraries: tqdm

With the dependencies installed and the 'training_data.csv' in the directory (lexicon.txt as well if running the lexicon-based model), the models can be run. When running, expect to see each of the processing steps displayed as they are completed and then a table of results (as seen in the results section). Parameters in the code can be tweaked to replicate experiments or for further experimentation. 
